{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the latest version of transformers\n",
    "!pip install -U git+https://github.com/huggingface/transformers.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"facebook/detr-resnet-50\"\n",
    "dataset_checkpoint = \"Yorai/detect-waste\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(dataset_checkpoint)\n",
    "ds_train = ds[\"train\"]\n",
    "ds_test = ds[\"test\"]\n",
    "remove_idx = [530] # negative bounding box\n",
    "keep = [i for i in range(len(ds_train)) if i not in remove_idx]\n",
    "ds_train = ds_train.select(keep)\n",
    "\n",
    "categories = ds_train.features[\"objects\"].feature[\"category\"].names\n",
    "\n",
    "id2label = {index: x for index, x in enumerate(categories, start=0)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from statistics import mean\n",
    "import random\n",
    "from datasets import interleave_datasets\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "\n",
    "if not Path(\"chosen.pkl\").exists():\n",
    "    categories_indexes = defaultdict(set)\n",
    "    for i,example in enumerate(ds_train):\n",
    "        for category in example[\"objects\"][\"category\"]:\n",
    "            categories_indexes[category].add(i)\n",
    "    k = int(mean(len(indexes) for indexes in categories_indexes.values()))\n",
    "    chosen = {}\n",
    "    for category, indexes in categories_indexes.items():\n",
    "        chosen[category] = random.choices(list(indexes), k=k)\n",
    "\n",
    "    dataset = interleave_datasets([ds_train.select(chosen[category]) for category in chosen.keys()])\n",
    "    print(\"Saving chosen\")\n",
    "    with open(\"chosen.pkl\", \"wb\") as f:\n",
    "        pkl.dump(chosen, f)\n",
    "else:\n",
    "    print(\"Loading chosen\")\n",
    "    with open(\"chosen.pkl\", \"rb\") as f:\n",
    "        chosen = pkl.load(f)\n",
    "    dataset = interleave_datasets([ds_train.select(chosen[category]) for category in chosen.keys()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "transform = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Resize(1024, 1024),\n",
    "        albumentations.HorizontalFlip(p=0.3),\n",
    "        albumentations.Flip(p=0.3),\n",
    "        albumentations.RandomBrightnessContrast(p=0.3),\n",
    "    ],\n",
    "    bbox_params=albumentations.BboxParams(format=\"coco\", label_fields=[\"category\"]),\n",
    ")\n",
    "def formatted_anns(image_id, category, area, bbox):\n",
    "    annotations = []\n",
    "    for i in range(0, len(category)):\n",
    "        new_ann = {\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": category[i],\n",
    "            \"isCrowd\": 0,\n",
    "            \"area\": area[i],\n",
    "            \"bbox\": list(bbox[i]),\n",
    "        }\n",
    "        annotations.append(new_ann)\n",
    "\n",
    "    return annotations\n",
    "def transform_aug_ann(examples):\n",
    "    image_ids = examples[\"image_id\"]\n",
    "    images, bboxes, area, categories = [], [], [], []\n",
    "    for image, objects in zip(examples[\"image\"], examples[\"objects\"]):\n",
    "        image = np.array(image.convert(\"RGB\"))[:, :, ::-1]\n",
    "        \n",
    "        out = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"category\"])\n",
    "\n",
    "        area.append(objects[\"area\"])\n",
    "        images.append(out[\"image\"])\n",
    "        bboxes.append(out[\"bboxes\"])\n",
    "        categories.append(out[\"category\"])\n",
    "\n",
    "    targets = [\n",
    "        {\"image_id\": id_, \"annotations\": formatted_anns(id_, cat_, ar_, box_)}\n",
    "        for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)\n",
    "    ]\n",
    "\n",
    "    return image_processor(images=images, annotations=targets, return_tensors=\"pt\")\n",
    "def collate_fn(batch):\n",
    "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "    # encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    batch = {}\n",
    "    batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n",
    "    if \"pixel_mask\" in encoding:\n",
    "        batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "split_ds_train = ds_train.train_test_split(test_size=0.1, seed=42, shuffle=True).with_transform(transform_aug_ann)\n",
    "ds_train_augmented = split_ds_train[\"train\"]\n",
    "ds_val_augmented = split_ds_train[\"test\"]\n",
    "ds_test_augmented = ds_test.with_transform(transform_aug_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    num_queries=10,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, EarlyStoppingCallback, EvalPrediction\n",
    "from transformers.models.detr.modeling_detr import DetrObjectDetectionOutput\n",
    "import functools\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "def compute_metrics(eval_pred: EvalPrediction, meanAveragePrecision: MeanAveragePrecision):\n",
    "    # eval_pred is a tuple(tuple(outputs.values()),labels)\n",
    "    (loss_dict, scores, pred_boxes, last_hidden_state, encoder_last_hidden_state), labels = eval_pred # detr\n",
    "    predictions = []\n",
    "    for score, box in zip(scores, pred_boxes):\n",
    "        # Extract the bounding boxes, labels, and scores from the model's output\n",
    "        pred_scores = torch.from_numpy(score[:, :-1])  # Exclude the no-object class\n",
    "        pred_boxes = torch.from_numpy(box)\n",
    "        pred_labels = torch.argmax(pred_scores, dim=-1)\n",
    "\n",
    "        # Get the scores corresponding to the predicted labels\n",
    "        pred_scores_for_labels = torch.gather(pred_scores, 1, pred_labels.unsqueeze(-1)).squeeze(-1)\n",
    "        predictions.append(\n",
    "            {\n",
    "                \"boxes\": pred_boxes,\n",
    "                \"scores\": pred_scores_for_labels,\n",
    "                \"labels\": pred_labels,\n",
    "            }\n",
    "        )\n",
    "    target = list(map(lambda label: {\"boxes\": torch.from_numpy(label[\"boxes\"]), \"labels\": torch.from_numpy(label[\"class_labels\"])},sum(labels,[])))\n",
    "    meanAveragePrecision.update(preds=predictions, target=target)\n",
    "    results = meanAveragePrecision.compute()\n",
    "    # Convert tensors to scalars/lists, MLFlow doesn't really like tensors\n",
    "    results = {k: v.tolist() if isinstance(v, torch.Tensor) else v for k, v in results.items()}\n",
    "    return results\n",
    "\n",
    "mAP = MeanAveragePrecision(box_format=\"cxcywh\", class_metrics=True)\n",
    "compute_metrics = functools.partial(compute_metrics, meanAveragePrecision=mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, IntervalStrategy\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"finetuned_\"+checkpoint,\n",
    "    evaluation_strategy=IntervalStrategy.STEPS,\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"map_50\",\n",
    "    greater_is_better=True,\n",
    "    auto_find_batch_size=True,\n",
    "    dataloader_drop_last = True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=100,\n",
    "    dataloader_num_workers=8,\n",
    "    learning_rate=5e-5,\n",
    "    remove_unused_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, EarlyStoppingCallback, EvalPrediction\n",
    "\n",
    "import contextlib\n",
    "import copy\n",
    "import functools\n",
    "import glob\n",
    "import importlib.metadata\n",
    "import inspect\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "\n",
    "# Integrations must be imported before ML frameworks:\n",
    "# isort: off\n",
    "from transformers.integrations import (\n",
    "    get_reporting_integration_callbacks,\n",
    "    hp_params,\n",
    "    is_fairscale_available,\n",
    ")\n",
    "\n",
    "import huggingface_hub.utils as hf_hub_utils\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from huggingface_hub import Repository, create_repo, upload_folder\n",
    "from packaging import version\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import contextlib\n",
    "import copy\n",
    "import functools\n",
    "import glob\n",
    "import importlib.metadata\n",
    "import inspect\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "from transformers.data.data_collator import DataCollator, DataCollatorWithPadding, default_data_collator\n",
    "from transformers.debug_utils import DebugOption, DebugUnderflowOverflow\n",
    "from transformers.dependency_versions_check import dep_version_check\n",
    "from transformers.hyperparameter_search import ALL_HYPERPARAMETER_SEARCH_BACKENDS, default_hp_search_backend\n",
    "from transformers.integrations.deepspeed import deepspeed_init, deepspeed_load_checkpoint\n",
    "from transformers.modelcard import TrainingSummary\n",
    "from transformers.modeling_utils import PreTrainedModel, load_sharded_checkpoint, unwrap_model\n",
    "from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES, MODEL_MAPPING_NAMES\n",
    "from transformers.optimization import Adafactor, get_scheduler\n",
    "from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from transformers.trainer_callback import (\n",
    "    CallbackHandler,\n",
    "    DefaultFlowCallback,\n",
    "    PrinterCallback,\n",
    "    ProgressCallback,\n",
    "    TrainerCallback,\n",
    "    TrainerControl,\n",
    "    TrainerState,\n",
    ")\n",
    "from transformers.trainer_pt_utils import (\n",
    "    DistributedTensorGatherer,\n",
    "    IterableDatasetShard,\n",
    "    LabelSmoother,\n",
    "    LengthGroupedSampler,\n",
    "    SequentialDistributedSampler,\n",
    "    distributed_broadcast_scalars,\n",
    "    distributed_concat,\n",
    "    find_batch_size,\n",
    "    get_model_param_count,\n",
    "    get_module_class_from_name,\n",
    "    get_parameter_names,\n",
    "    nested_concat,\n",
    "    nested_detach,\n",
    "    nested_numpify,\n",
    "    nested_xla_mesh_reduce,\n",
    "    reissue_pt_warnings,\n",
    "    remove_dummy_checkpoint,\n",
    ")\n",
    "from transformers.trainer_utils import (\n",
    "    PREFIX_CHECKPOINT_DIR,\n",
    "    BestRun,\n",
    "    EvalLoopOutput,\n",
    "    EvalPrediction,\n",
    "    FSDPOption,\n",
    "    HPSearchBackend,\n",
    "    HubStrategy,\n",
    "    IntervalStrategy,\n",
    "    PredictionOutput,\n",
    "    RemoveColumnsCollator,\n",
    "    ShardedDDPOption,\n",
    "    TrainerMemoryTracker,\n",
    "    TrainOutput,\n",
    "    default_compute_objective,\n",
    "    denumpify_detensorize,\n",
    "    enable_full_determinism,\n",
    "    find_executable_batch_size,\n",
    "    get_last_checkpoint,\n",
    "    has_length,\n",
    "    number_of_arguments,\n",
    "    seed_worker,\n",
    "    set_seed,\n",
    "    speed_metrics,\n",
    ")\n",
    "from transformers.training_args import OptimizerNames, ParallelMode, TrainingArguments\n",
    "from transformers.utils import (\n",
    "    ADAPTER_CONFIG_NAME,\n",
    "    ADAPTER_SAFE_WEIGHTS_NAME,\n",
    "    ADAPTER_WEIGHTS_NAME,\n",
    "    CONFIG_NAME,\n",
    "    SAFE_WEIGHTS_INDEX_NAME,\n",
    "    SAFE_WEIGHTS_NAME,\n",
    "    WEIGHTS_INDEX_NAME,\n",
    "    WEIGHTS_NAME,\n",
    "    PushInProgress,\n",
    "    can_return_loss,\n",
    "    find_labels,\n",
    "    is_accelerate_available,\n",
    "    is_apex_available,\n",
    "    is_bitsandbytes_available,\n",
    "    is_datasets_available,\n",
    "    is_in_notebook,\n",
    "    is_ipex_available,\n",
    "    is_peft_available,\n",
    "    is_safetensors_available,\n",
    "    is_sagemaker_dp_enabled,\n",
    "    is_sagemaker_mp_enabled,\n",
    "    is_torch_compile_available,\n",
    "    is_torch_neuroncore_available,\n",
    "    is_torch_tpu_available,\n",
    "    logging,\n",
    "    strtobool,\n",
    ")\n",
    "from transformers.utils.quantization_config import QuantizationMethod\n",
    "\n",
    "\n",
    "DEFAULT_CALLBACKS = [DefaultFlowCallback]\n",
    "DEFAULT_PROGRESS_CALLBACK = ProgressCallback\n",
    "\n",
    "if is_in_notebook():\n",
    "    from transformers.utils.notebook import NotebookProgressCallback\n",
    "\n",
    "    DEFAULT_PROGRESS_CALLBACK = NotebookProgressCallback\n",
    "\n",
    "if is_apex_available():\n",
    "    from apex import amp\n",
    "\n",
    "if is_datasets_available():\n",
    "    import datasets\n",
    "\n",
    "if is_torch_tpu_available(check_device=False):\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.debug.metrics as met\n",
    "\n",
    "if is_fairscale_available():\n",
    "    dep_version_check(\"fairscale\")\n",
    "    import fairscale\n",
    "    from fairscale.nn.data_parallel import FullyShardedDataParallel as FullyShardedDDP\n",
    "    from fairscale.nn.data_parallel import ShardedDataParallel as ShardedDDP\n",
    "    from fairscale.nn.wrap import auto_wrap\n",
    "    from fairscale.optim import OSS\n",
    "    from fairscale.optim.grad_scaler import ShardedGradScaler\n",
    "\n",
    "\n",
    "if is_sagemaker_mp_enabled():\n",
    "    import smdistributed.modelparallel.torch as smp\n",
    "    from smdistributed.modelparallel import __version__ as SMP_VERSION\n",
    "\n",
    "    IS_SAGEMAKER_MP_POST_1_10 = version.parse(SMP_VERSION) >= version.parse(\"1.10\")\n",
    "\n",
    "    from transformers.trainer_pt_utils import smp_forward_backward, smp_forward_only, smp_gather, smp_nested_concat\n",
    "else:\n",
    "    IS_SAGEMAKER_MP_POST_1_10 = False\n",
    "\n",
    "\n",
    "if is_safetensors_available():\n",
    "    import safetensors.torch\n",
    "\n",
    "\n",
    "if is_peft_available():\n",
    "    from peft import PeftModel\n",
    "\n",
    "\n",
    "if is_accelerate_available():\n",
    "    from accelerate import Accelerator, skip_first_batches\n",
    "    from accelerate import __version__ as accelerate_version\n",
    "    from accelerate.utils import DistributedDataParallelKwargs, GradientAccumulationPlugin\n",
    "\n",
    "    if version.parse(accelerate_version) > version.parse(\"0.20.3\"):\n",
    "        from accelerate.utils import (\n",
    "            load_fsdp_model,\n",
    "            load_fsdp_optimizer,\n",
    "            save_fsdp_model,\n",
    "            save_fsdp_optimizer,\n",
    "        )\n",
    "\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    import optuna\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "# Name of the files used for checkpointing\n",
    "TRAINING_ARGS_NAME = \"training_args.bin\"\n",
    "TRAINER_STATE_NAME = \"trainer_state.json\"\n",
    "OPTIMIZER_NAME = \"optimizer.pt\"\n",
    "OPTIMIZER_NAME_BIN = \"optimizer.bin\"\n",
    "SCHEDULER_NAME = \"scheduler.pt\"\n",
    "SCALER_NAME = \"scaler.pt\"\n",
    "\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def evaluation_loop(\n",
    "        self,\n",
    "        dataloader: DataLoader,\n",
    "        description: str,\n",
    "        prediction_loss_only: Optional[bool] = None,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "        metric_key_prefix: str = \"eval\",\n",
    "    ) -> EvalLoopOutput:\n",
    "        \"\"\"\n",
    "        Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n",
    "\n",
    "        Works both with or without labels.\n",
    "        \"\"\"\n",
    "        args = self.args\n",
    "\n",
    "        prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else args.prediction_loss_only\n",
    "\n",
    "        # if eval is called w/o train, handle model prep here\n",
    "        if self.is_deepspeed_enabled and self.deepspeed is None:\n",
    "            _, _ = deepspeed_init(self, num_training_steps=0, inference=True)\n",
    "\n",
    "        model = self._wrap_model(self.model, training=False, dataloader=dataloader)\n",
    "\n",
    "        if len(self.accelerator._models) == 0 and model is self.model:\n",
    "            model = (\n",
    "                self.accelerator.prepare(model)\n",
    "                if self.is_deepspeed_enabled\n",
    "                else self.accelerator.prepare_model(model, evaluation_mode=True)\n",
    "            )\n",
    "\n",
    "            if self.is_fsdp_enabled:\n",
    "                self.model = model\n",
    "\n",
    "            # for the rest of this function `model` is the outside model, whether it was wrapped or not\n",
    "            if model is not self.model:\n",
    "                self.model_wrapped = model\n",
    "\n",
    "            # backward compatibility\n",
    "            if self.is_deepspeed_enabled:\n",
    "                self.deepspeed = self.model_wrapped\n",
    "\n",
    "        # if full fp16 or bf16 eval is wanted and this ``evaluation`` or ``predict`` isn't called\n",
    "        # while ``train`` is running, cast it to the right dtype first and then put on device\n",
    "        if not self.is_in_train:\n",
    "            if args.fp16_full_eval:\n",
    "                model = model.to(dtype=torch.float16, device=args.device)\n",
    "            elif args.bf16_full_eval:\n",
    "                model = model.to(dtype=torch.bfloat16, device=args.device)\n",
    "\n",
    "        batch_size = self.args.eval_batch_size\n",
    "\n",
    "        logger.info(f\"***** Running {description} *****\")\n",
    "        if has_length(dataloader):\n",
    "            logger.info(f\"  Num examples = {self.num_examples(dataloader)}\")\n",
    "        else:\n",
    "            logger.info(\"  Num examples: Unknown\")\n",
    "        logger.info(f\"  Batch size = {batch_size}\")\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        self.callback_handler.eval_dataloader = dataloader\n",
    "        # Do this before wrapping.\n",
    "        eval_dataset = getattr(dataloader, \"dataset\", None)\n",
    "\n",
    "        if args.past_index >= 0:\n",
    "            self._past = None\n",
    "\n",
    "        # Initialize containers\n",
    "        # losses/preds/labels on GPU/TPU (accumulated for eval_accumulation_steps)\n",
    "        losses_host = None\n",
    "        preds_host = None\n",
    "        labels_host = None\n",
    "        inputs_host = None\n",
    "\n",
    "        # losses/preds/labels on CPU (final containers)\n",
    "        all_losses = None\n",
    "        all_preds = None\n",
    "        all_labels = None\n",
    "        all_inputs = None\n",
    "        # Will be useful when we have an iterable dataset so don't know its length.\n",
    "\n",
    "        observed_num_examples = 0\n",
    "        # Main evaluation loop\n",
    "        for step, inputs in enumerate(dataloader):\n",
    "            # Update the observed num examples\n",
    "            observed_batch_size = find_batch_size(inputs)\n",
    "            if observed_batch_size is not None:\n",
    "                observed_num_examples += observed_batch_size\n",
    "                # For batch samplers, batch_size is not known by the dataloader in advance.\n",
    "                if batch_size is None:\n",
    "                    batch_size = observed_batch_size\n",
    "\n",
    "            # Prediction step\n",
    "            loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
    "            main_input_name = getattr(self.model, \"main_input_name\", \"input_ids\")\n",
    "            inputs_decode = self._prepare_input(inputs[main_input_name]) if args.include_inputs_for_metrics else None\n",
    "\n",
    "            if is_torch_tpu_available():\n",
    "                xm.mark_step()\n",
    "\n",
    "            # Update containers on host\n",
    "            if loss is not None:\n",
    "                losses = self.accelerator.gather_for_metrics((loss.repeat(batch_size)))\n",
    "                losses_host = losses if losses_host is None else nested_concat(losses_host, losses, padding_index=-100)\n",
    "            if labels is not None:\n",
    "                labels = self.accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "            if inputs_decode is not None:\n",
    "                inputs_decode = self.accelerator.pad_across_processes(inputs_decode, dim=1, pad_index=-100)\n",
    "                inputs_decode = self.accelerator.gather_for_metrics((inputs_decode))\n",
    "                inputs_host = (\n",
    "                    inputs_decode\n",
    "                    if inputs_host is None\n",
    "                    else nested_concat(inputs_host, inputs_decode, padding_index=-100)\n",
    "                )\n",
    "            if logits is not None:\n",
    "                logits = self.accelerator.pad_across_processes(logits, dim=1, pad_index=-100)\n",
    "                if self.preprocess_logits_for_metrics is not None:\n",
    "                    logits = self.preprocess_logits_for_metrics(logits, labels)\n",
    "                logits = self.accelerator.gather_for_metrics((logits))\n",
    "                preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)\n",
    "\n",
    "            if labels is not None:\n",
    "                labels = self.accelerator.gather_for_metrics((labels))\n",
    "                labels_host = [labels] if labels_host is None else labels_host+[labels]\n",
    "                \n",
    "                # labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)\n",
    "\n",
    "            self.control = self.callback_handler.on_prediction_step(args, self.state, self.control)\n",
    "\n",
    "            # Gather all tensors and put them back on the CPU if we have done enough accumulation steps.\n",
    "            if (\n",
    "                args.eval_accumulation_steps is not None\n",
    "                and (step + 1) % args.eval_accumulation_steps == 0\n",
    "                and self.accelerator.sync_gradients\n",
    "            ):\n",
    "                if losses_host is not None:\n",
    "                    losses = nested_numpify(losses_host)\n",
    "                    all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n",
    "                if preds_host is not None:\n",
    "                    logits = nested_numpify(preds_host)\n",
    "                    all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n",
    "                if inputs_host is not None:\n",
    "                    inputs_decode = nested_numpify(inputs_host)\n",
    "                    all_inputs = (\n",
    "                        inputs_decode\n",
    "                        if all_inputs is None\n",
    "                        else nested_concat(all_inputs, inputs_decode, padding_index=-100)\n",
    "                    )\n",
    "                if labels_host is not None:\n",
    "                    labels = nested_numpify(labels_host)\n",
    "                    all_labels = [labels] if all_labels is None else all_labels+[labels]\n",
    "                    \n",
    "                    # all_labels = (\n",
    "                    #     labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n",
    "                    # )\n",
    "\n",
    "                # Set back to None to begin a new accumulation\n",
    "                losses_host, preds_host, inputs_host, labels_host = None, None, None, None\n",
    "\n",
    "        if args.past_index and hasattr(self, \"_past\"):\n",
    "            # Clean the state at the end of the evaluation loop\n",
    "            delattr(self, \"_past\")\n",
    "\n",
    "        # Gather all remaining tensors and put them back on the CPU\n",
    "        if losses_host is not None:\n",
    "            losses = nested_numpify(losses_host)\n",
    "            all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n",
    "        if preds_host is not None:\n",
    "            logits = nested_numpify(preds_host)\n",
    "            all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n",
    "        if inputs_host is not None:\n",
    "            inputs_decode = nested_numpify(inputs_host)\n",
    "            all_inputs = (\n",
    "                inputs_decode if all_inputs is None else nested_concat(all_inputs, inputs_decode, padding_index=-100)\n",
    "            )\n",
    "        if labels_host is not None:\n",
    "            labels = nested_numpify(labels_host)\n",
    "            all_labels = labels if all_labels is None else all_labels+labels\n",
    "            # all_labels = labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n",
    "\n",
    "        # Number of samples\n",
    "        if has_length(eval_dataset):\n",
    "            num_samples = len(eval_dataset)\n",
    "        # The instance check is weird and does not actually check for the type, but whether the dataset has the right\n",
    "        # methods. Therefore we need to make sure it also has the attribute.\n",
    "        elif isinstance(eval_dataset, IterableDatasetShard) and getattr(eval_dataset, \"num_examples\", 0) > 0:\n",
    "            num_samples = eval_dataset.num_examples\n",
    "        else:\n",
    "            if has_length(dataloader):\n",
    "                num_samples = self.num_examples(dataloader)\n",
    "            else:  # both len(dataloader.dataset) and len(dataloader) fail\n",
    "                num_samples = observed_num_examples\n",
    "        if num_samples == 0 and observed_num_examples > 0:\n",
    "            num_samples = observed_num_examples\n",
    "\n",
    "        # Metrics!\n",
    "        if self.compute_metrics is not None and all_preds is not None and all_labels is not None:\n",
    "            if args.include_inputs_for_metrics:\n",
    "                metrics = self.compute_metrics(\n",
    "                    EvalPrediction(predictions=all_preds, label_ids=all_labels, inputs=all_inputs)\n",
    "                )\n",
    "            else:\n",
    "                metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels))\n",
    "        else:\n",
    "            metrics = {}\n",
    "\n",
    "        # To be JSON-serializable, we need to remove numpy types or zero-d tensors\n",
    "        metrics = denumpify_detensorize(metrics)\n",
    "\n",
    "        if all_losses is not None:\n",
    "            metrics[f\"{metric_key_prefix}_loss\"] = all_losses.mean().item()\n",
    "        if hasattr(self, \"jit_compilation_time\"):\n",
    "            metrics[f\"{metric_key_prefix}_jit_compilation_time\"] = self.jit_compilation_time\n",
    "\n",
    "        # Prefix all keys with metric_key_prefix + '_'\n",
    "        for key in list(metrics.keys()):\n",
    "            if not key.startswith(f\"{metric_key_prefix}_\"):\n",
    "                metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n",
    "\n",
    "        return EvalLoopOutput(predictions=all_preds, label_ids=all_labels, metrics=metrics, num_samples=num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=ds_train_augmented,\n",
    "    eval_dataset=ds_val_augmented,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)],\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "training_output = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
